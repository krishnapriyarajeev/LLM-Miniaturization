#Generative AI and LLM Course - Deep Learning
Foundation models with billions of parameters, exhibit emergent properties beyond language alone, and researchers are unlocking their ability to break down complex tasks, reason, and problem solve. These are parameters can be thought of as a model’s memory. And the more parameters a model has, the more memory, and as it turns out, the more sophisticated the tasks it can perform. 

The text that you pass to an LLM is known as a prompt. The space or memory that is available to the prompt is called the context window, and this is typically large enough for a few thousand words, but differs from model to model. The prompt is passed to the model, the model then predicts the next words, and because your prompt contained a question, this model generates an answer. The completion is comprised of the text contained in the original prompt, followed by the generated text. The output of the model is called a completion, and the act of using the model to generate text is known as inference. 

You can ask the model to identify all of the people and places identified in a news article. This is known as named entity recognition, a word classification. Developers have discovered that as the scale of foundation models grows from hundreds of millions of parameters to billions, even hundreds of billions, the subjective understanding of language that a model possesses also increases. This language understanding stored within the parameters of the model is what processes, reasons, and ultimately solves the tasks you give it, but it's also true that smaller models can be fine tuned to perform well on specific focused tasks.

RNNs while powerful for their time, were limited by the amount of compute and memory needed to perform well at generative tasks. The problem here is that language is complex. In many languages, one word can have multiple meanings. These are homonyms. 

Attention is All You Need. Transformers can be scaled efficiently to use multi-core GPUs, it can parallel process input data, making use of much larger training datasets, and crucially, it's able to learn to pay attention to the meaning of the words it's processing. The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence.

Words to note- attention map, attention weights

The transformer architecture is split into two distinct parts, the encoder and the decoder. These components work in conjunction with each other and they share a number of similarities. 


#Transformer’s architecture
This diagram is called an attention map and can be useful to illustrate the attention weights between each word and every other word. Check self-attention.

Machine-learning models are just big statistical calculators and they work with numbers, not words. So before passing texts into the model to process, you must first tokenize the words. Simply put, this converts the words into numbers, with each number representing a position in a dictionary of all the possible words that the model can work with.
This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space.
Each token ID in the vocabulary is matched to a multi-dimensional vector, and the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence, ie, each word has been matched to a token ID, and each token is mapped into a vector.
In the original transformer paper, the vector size was actually 512, so much bigger than we can fit onto this image. For simplicity, if you imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words.
The model processes each of the input tokens in parallel. So by adding the positional encoding, you preserve the information about the word order and don't lose the relevance of the position of the word in the sentence. Once you've summed the input tokens and the positional encodings, you pass the resulting vectors to the self-attention layer. Here, the model analyzes the relationships between the tokens in your input sequence. 
The self-attention weights that are learned during training gives us the importance of each word in that sentence. Usually, it has multiple sets of self-attention layers to capture different aspects of the language. It is processed parallely and independently
Once all the attention weights have been applied to input data, the output is processed through a fully connected feed forward network.
The output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary.This is then passed through a softmax function where its normalized into a probability score for each word. The token with the highest probability is the value we need.


#What happens in decoder
The task of the Transformer decoder is to complete text, much like GPT models.
The input consists of a sequence of tokens (e.g., "it's a blue"), and the goal is to predict the next word (e.g., "sundress").
The output is a probability distribution over potential next tokens.
Inference involves sampling a token from the distribution, appending it to the input, and running the model again with the updated input.
ChatGPT operates by seeing user input, sampling the next word, appending it, and repeating this process
